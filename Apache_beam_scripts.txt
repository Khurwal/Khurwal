C:\Users\AshwaniKumar\AppData\Local\pip

python -m apache_beam.examples.wordcount --input test.txt --output count.txt

import apache_beam as beam
from apache_beam.io import ReadFromText
from apache_beam.io import WriteToText
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.options.pipeline_options import SetupOptions
with beam.Pipeline() as pipeline:
	lines = pipeline | 'ReadMyFile' >> beam.io.ReadFromText('test.txt')

class ComputeWordLengthFn(beam.DoFn):
  def process(self, element):
    return [len(element)]

word_lengths = lines | beam.ParDo(ComputeWordLengthFn())


with beam.Pipeline() as pipeline:
    lines = pipeline | beam.Create(["this is test", "this is another test"])
    word_count = (lines 
                  | "Word" >> beam.ParDo(lambda line: line.split(" "))
                  | "Pair of One" >> beam.Map(lambda w: (w, 1))
                  | "Group" >> beam.GroupByKey()
                  | "Count" >> beam.Map(lambda (w, o): (w, sum(o))))
		  | "print" >> beam.Map(print)
    result = pipeline.run()




++++++++++++++++++++++++++++++BEAM WORD COUNT

import apache_beam as beam

# lets have a sample string
data = ["this is sample data", "this is yet another sample data"]

# create a pipeline
pipeline = beam.Pipeline()
counts = (pipeline | "create" >> beam.Create(data)
    | "split" >> beam.ParDo(lambda row: row.split(" "))
    | "pair" >> beam.Map(lambda w: (w, 1))
    | "group" >> beam.CombinePerKey(sum))

# lets collect our result with a map transformation into output array
output = []
def collect(row):
    output.append(row)
    return True

counts | "print" >> beam.Map(collect)

# Run the pipeline
result = pipeline.run()

# lets wait until result a available
result.wait_until_finish()

# print the output
print output

+++++++++++++++++++++++++++++++++++++++++++++++
data1 = ["a", "b", "c", "b", "c", "d"]
pipeline = beam.Pipeline()
count1= (pipeline | "create_data1" >> beam.Create(data1)| "split_Data1" >> beam.ParDo(lambda row: row.split(" "))|"pair_data1" >> beam.Map(lambda w: (w, 1))| "group" >> beam.CombinePerKey(sum))
count1| beam.ParDo(lambda x: print(x))
result = pipeline.run()


+++++++++++++++++++++++++++++++++++++++
str=" my name is ASHWANI"
data1=list(str.split(" "))
pipeline = beam.Pipeline()
count1= (pipeline | "create_data1" >> beam.Create(data1)| "split_Data1" >> beam.ParDo(lambda row: row.split(" "))|"Filter" >> beam.Filter(lambda x: x=='my')|"pair_data1" >> beam.Map(lambda w: (w, 1))| "group" >> beam.CombinePerKey(sum))
count1| beam.ParDo(lambda x: print(x))
result = pipeline.run()


+++++++++++++++++++++++++++++++++++++++++++++++++++++++

from apache_beam.io import ReadFromPubSub
from apache_beam.io import WritetoPubSub

topic_name = "projects/pubsub-public-data/topics/taxirides-realtime"

options = pipeline_options.PipelineOptions()
options.view_as(pipeline_options.StandardOptions).streaming = True
_, options.view_as(GoogleCloudOptions).project = google.auth.default()

ib.options.recording_duration = '18s'

class dataAsKey(beam.DoFn):
    def process(self, element, window=beam.DoFn.WindowParam):
         yield (format(window.start.to_utc_datetime()) , 1)

p = beam.Pipeline(interactive_runner.InteractiveRunner(), options=options)

data = (p 
    | "Read" >> beam.io.ReadFromPubSub(topic=topic_name) 
    | 'Window' >> beam.WindowInto(beam.window.FixedWindows(6))
)
ib.show(data)

count = data(
    | 'Data as key' >> beam.ParDo(dataAsKey())
    | 'Count per Window' >> Count.PerKey()

ib.show(count)


